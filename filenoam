from collections import defaultdict

def min_max_normalize(dictionary):
    min_value = min(dictionary.values())
    max_value = max(dictionary.values())
    denominator = max_value - min_value
    if denominator == 0:
        return dictionary
    
    for key in dictionary:
        dictionary[key] = (dictionary[key] - min_value) / denominator
    return dictionary

def search_anchor(query, index):
    dict_cosine_sim = defaultdict(float)
    query_dict = dict(term_frequency(query, 0))
    query_terms_set = set(query_dict.keys())
    index_df_set = set(index.df.keys())

    # Only process terms that exist in both the query and index
    common_terms = query_terms_set.intersection(index_df_set)

    for term in common_terms:
        posting_list = index.read_a_posting_list(".", term, "noam209263805")
        for doc_id, freq in posting_list:
            dict_cosine_sim[doc_id] += freq

    # Normalize the values in dict_cosine_sim
    dict_cosine_sim_normalized = min_max_normalize(dict_cosine_sim)

    # Sort and select the top 100 documents
    sorted_docs = sorted(dict_cosine_sim_normalized.items(), key=lambda x: x[1], reverse=True)
    top_100_docs = sorted_docs[:100]

    return top_100_docs
