
##v try to create df*idf dict foe each word -- noam -----------------------------------------------------------------------

# Function to calculate TF for a document
def term_frequency(text):
    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]

    # Count the frequency of each word that is not a stopword
    word_freq = {}
    for token in tokens:
        if token not in all_stopwords:
            if token not in word_freq:
                word_freq[token] = 1
            else:
                word_freq[token] += 1

    return [(token, freq) for token, freq in word_freq.items()]

# Calculate TF for each document
tf_rdd = doc_body_pairs.map(lambda x: Row(id=x["id"], tf=term_frequency(x["text"])))

# Calculate IDF for each term
# Assuming postings_filtered is an RDD containing term-document frequency pairs
w2df = calculate_df(postings_filtered)
w2df_dict = w2df.collectAsMap()

# Calculate the number of documents
N = doc_body_pairs.count()

# Function to calculate TF-IDF for a document
def calculate_tf_idf(tf):
    tfidf_scores = {}
    for term, freq in tf:
        idf = inverse_document_frequency(N, w2df_dict.get(term, 1))  # Use 1 as default for unseen terms
        tfidf_scores[term] = freq * idf
    return tfidf_scores

# Calculate TF-IDF for each document
tfidf_rdd = tf_rdd.map(lambda x: Row(id=x["id"], tfidf=calculate_tf_idf(x["tf"])))

# Aggregate TF-IDF scores for each word across all documents
word_tfidf_sum = tfidf_rdd.flatMap(lambda x: [(term, score) for term, score in x["tfidf"].items()]) \
                           .reduceByKey(lambda x, y: x + y) \
                           .collectAsMap()

# Print the resulting dictionary
for word, tfidf_sum in word_tfidf_sum.items():
    print(f"{word}: {tfidf_sum}")
